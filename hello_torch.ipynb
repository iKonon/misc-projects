{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hands-on Introduction to Torch\n",
    "\n",
    "Edited by Ronan Collobert<br>\n",
    "original version by Soumith Chintala\n",
    "\n",
    "[Get started](http://ronan.collobert.com/torch/intro.pdf)<br>\n",
    "[Get this itorch notebook](http://ronan.collobert.com/torch/Deep Learning with Torch.ipynb)\n",
    "\n",
    "Run itorch\n",
    "\n",
    "```sh\n",
    "itorch notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of this talk\n",
    "* Understand torch and the neural networks package at a high-level.\n",
    "* Train a small neural network on CPU and GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Torch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch is an scientific computing framework based on Lua[JIT] with strong CPU and CUDA backends.\n",
    "\n",
    "Strong points of Torch:\n",
    "\n",
    "* Efficient Tensor library (like NumPy) with an efficient CUDA backend\n",
    "* Neural Networks package -- build arbitrary acyclic computation graphs with automatic differentiation\n",
    "   * also with fast CUDA and CPU backends\n",
    "* Good community and industry support - several hundred community-built and maintained packages.\n",
    "* Easy to use Multi-GPU support and parallelizing neural networks\n",
    "\n",
    "[http://torch.ch](http://torch.ch)<br>\n",
    "[https://github.com/torch/torch7/wiki/Cheatsheet](https://github.com/torch/torch7/wiki/Cheatsheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based on [Lua](http://lua.org) and C.\n",
    "* Currently runs on [LuaJIT](http://luajit.org) (Just-in-time compiler) which is fast and supports FFI.\n",
    "* Lua is pretty close to javascript.\n",
    "   * variables are global by default, unless `local` keyword is used\n",
    "   * Only has one data structure built-in, a table: `{}`. Doubles as a hash-table and an array.\n",
    "   * 1-based indexing.\n",
    "   * `foo:bar()` is the same as `foo.bar(foo)`\n",
    "   \n",
    "* Lua __glues__ C/C++ libraries together\n",
    "   * __Develop__ fast (scripting language), __run__ fast (minor overhead, C backend)\n",
    "   \n",
    "* The basic brick is the __Tensor__ object\n",
    "   * n-dimensional array\n",
    "   * used to store any kind of data\n",
    "   \n",
    "* The __torch__ package provides tensors... _hundred_ of packages are built upon it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "#### Lua Types\n",
    "\n",
    "Lua has 8 main types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(nil))\n",
    "print(type(true))\n",
    "print(type(10.4*3))\n",
    "print(type(\"Hello world\"))\n",
    "print(type(function() print(\"Hello world\") end))\n",
    "print(type({a=3, b=4}))\n",
    "print(type(torch.Tensor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `thread` type will not be covered by this tutorial.\n",
    "Note that `userdata` allows to create C objects (like several Torch objects), and define your own type system over them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strings, numbers, tables - a tiny introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 'hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b[1] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b[2] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i=1,#b do -- the # operator is the length operator in Lua\n",
    "    print(b[i]) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = torch.Tensor(5,3) -- construct a 5x3 matrix, uninitialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = torch.rand(5,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Views\n",
    "A tensor is a view over a piece of memory (a storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(a:storage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch relies heavily on views:\n",
    "  - narrow(dim, idx, size)\n",
    "  - select(dim, idx)\n",
    "  - unfold(dim, kw, dw)\n",
    "  - view(dim1, dim2, dim3, ...)\n",
    "  - index operator [{}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(a:narrow(1, 3, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that view = *pointer* in a storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "an = a:narrow(1, 3, 2)\n",
    "an:zero()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a:select(2, 2):fill(3.14)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(a:size())\n",
    "print(a:stride())\n",
    "print(a:storageOffset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(a:select(2, 2):size())\n",
    "print(a:select(2, 2):stride())\n",
    "print(a:select(2, 2):storageOffset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Math operations\n",
    "\n",
    "See [torch documentation](https://github.com/torch/torch7/blob/master/doc/maths.md)\n",
    "for a survey on available math operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b=torch.rand(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- matrix-matrix multiplication: syntax 1\n",
    "a*b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- matrix-matrix multiplication: syntax 2\n",
    "torch.mm(a,b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- matrix-matrix multiplication: syntax 3\n",
    "c=torch.Tensor(5,4)\n",
    "c:mm(a,b) -- store the result of a*b in c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA Tensors\n",
    "Tensors can be moved onto GPU using the :cuda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'cutorch';\n",
    "a = a:cuda()\n",
    "b = b:cuda()\n",
    "c = c:cuda()\n",
    "c:mm(a,b) -- done on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 5\n",
    "\n",
    "-- make sure everybody has the same random seed\n",
    "torch.manualSeed(1234)\n",
    "\n",
    "-- create a random NxN matrix\n",
    "A = torch.rand(N, N)\n",
    "\n",
    "-- make it symmetric positive\n",
    "A = A*A:t()\n",
    "\n",
    "-- make it definite\n",
    "A:add(0.001, torch.eye(N))\n",
    "\n",
    "-- add a linear term\n",
    "b = torch.rand(N)\n",
    "\n",
    "-- create a quadratic form\n",
    "function J(x)\n",
    "    return 0.5*x:dot(A*x)-b:dot(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function call, here at a random point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(J(torch.rand(N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: find the minimum of the quadratic function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can inverse the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs = torch.inverse(A)*b\n",
    "print(string.format('J(x^*) = %g', J(xs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or we can do a gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function dJ(x)\n",
    "  return A*x-b\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some current solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = torch.rand(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then apply gradient descent (with a given learning rate `lr`) for a while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "for i=1,20000 do\n",
    "  x = x - dJ(x)*lr\n",
    "  -- we print the value of the objective function every 1000 iterations\n",
    "  if i % 1000 == 0 then\n",
    "    print(string.format('at iter %d J(x) = %f', i, J(x)))\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Scalar & tensor arithmetic\n",
    "A = torch.eye(3)\n",
    "b = 4\n",
    "c = 2\n",
    "print(A*b - c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Max\n",
    "print(torch.max(torch.FloatTensor{1,3,5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Clamp\n",
    "torch.clamp(torch.range(0,4),0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Matrix multiply\n",
    "A = torch.eye(3)\n",
    "B = torch.ones(3,1)*3\n",
    "print(A*B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Boolean fns\n",
    "A = torch.range(1,5)\n",
    "print(torch.le(A,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scientific Computing Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Special functions\n",
    "require 'cephes'\n",
    "print(cephes.gamma(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(cephes.atan2(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Sampling from a distribution\n",
    "require 'randomkit'\n",
    "a = torch.zeros(10000)\n",
    "randomkit.negative_binomial(a,9,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Plot = require 'itorch.Plot'\n",
    "local p = Plot()\n",
    "    :histogram(a,80,1,80)\n",
    "    :title(\"Histogram of Draws From Negative Binomial\")\n",
    "    :draw();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory-layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.DoubleTensor(4, 6) -- DoubleTensor, uninitialized memory\n",
    "a:uniform() -- fills \"a\" with uniform noise with mean=0, stdev=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = a:select(1, 3) -- Select from the 1st axis (rows), \n",
    "                   -- the 3rd set of entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b:fill(3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(a) -- Look at the 3rd row! It's been filled with 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Arithmetic is no problem\n",
    "grad = require 'autograd'\n",
    "function f(a,b,c)\n",
    "    return a + b * c\n",
    "end\n",
    "df = grad(f)\n",
    "da, val = df(3.5, 2.1, 1.1)\n",
    "print(\"Value: \"..val)\n",
    "print(\"Gradient: \"..da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- If statements are no problem\n",
    "grad = require 'autograd'\n",
    "function f(a,b,c)\n",
    "    if b > c then\n",
    "        return a * math.sin(b)\n",
    "    else\n",
    "        return a + b * c\n",
    "    end\n",
    "end\n",
    "g = grad(f)\n",
    "da, val = g(3.5, 2.1, 1.1)\n",
    "print(\"Value: \"..val)\n",
    "print(\"Gradient: \"..da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Of course, works with tensors\n",
    "grad = require 'autograd'\n",
    "function f(a,b,c)\n",
    "    if torch.sum(b) > torch.sum(c) then\n",
    "        return torch.sum(torch.cmul(a,torch.sin(b)))\n",
    "    else\n",
    "        return torch.sum(a + torch.cmul(b,c))\n",
    "    end\n",
    "end\n",
    "g = grad(f)\n",
    "a = torch.randn(3,3)\n",
    "b = torch.eye(3,3)\n",
    "c = torch.randn(3,3)\n",
    "da, val = g(a,b,c)\n",
    "print(\"Value: \"..val)\n",
    "print(\"Gradient: \")\n",
    "print(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Autograd for loop\n",
    "function f(a,b)\n",
    "    for i=1,b do\n",
    "        a = a*a\n",
    "    end\n",
    "    return a\n",
    "end\n",
    "g = grad(f)\n",
    "da, val = g(3,2)\n",
    "print(\"Value: \"..val)\n",
    "print(\"Gradient: \"..da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Autograd recursive function\n",
    "function f(a,b)\n",
    "    if b == 0 then\n",
    "        return a\n",
    "    else\n",
    "        return f(a*a,b-1)\n",
    "    end\n",
    "end\n",
    "g = grad(f)\n",
    "da, val = g(3,2)\n",
    "print(\"Value: \"..val)\n",
    "print(\"Gradient: \"..da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- New ops aren't a problem\n",
    "function f(a)\n",
    "    return torch.sum(torch.floor(torch.pow(a,3)))\n",
    "end\n",
    "g = grad(f)\n",
    "da, val = g(torch.eye(3))\n",
    "print(\"Value: \"..val)\n",
    "print(\"Gradient:\")\n",
    "print(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- New ops aren't a problem\n",
    "grad = require 'autograd'\n",
    "special = {}\n",
    "special.floor = function(x) return torch.floor(x) end\n",
    "-- Overload our new mini-module, called \"special\"\n",
    "grad.overload.module(\"special\",special,function(module)\n",
    "    -- Define a gradient for the member function \"floor\"\n",
    "    module.gradient(\"floor\", {\n",
    "                -- Here's our new partial derivative\n",
    "                -- (if we had two arguments, \n",
    "                -- we'd define two functions)\n",
    "                function(g,ans,x) \n",
    "                    return g\n",
    "                end\n",
    "            })\n",
    "    end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function f(a)\n",
    "    return torch.sum(special.floor(torch.pow(a,3)))\n",
    "end\n",
    "g = grad(f)\n",
    "da, val = g(torch.eye(3))\n",
    "print(\"Value: \"..val)\n",
    "print(\"Gradient:\")\n",
    "print(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function f(a,b)\n",
    "    c = a * b\n",
    "    if c > 0 then\n",
    "        d = torch.log(c)\n",
    "    else\n",
    "        d = torch.sin(c)\n",
    "    end\n",
    "    return d\n",
    "end\n",
    "print(f(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function f(a,b,c)\n",
    "    if b > c then\n",
    "        d = a * math.sin(b)\n",
    "    else\n",
    "        d = a + b * c\n",
    "    end\n",
    "    return d\n",
    "end\n",
    "print(f(3,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad = require 'autograd'\n",
    "g = grad(f)\n",
    "print(g(3,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Representation of \"Wengert list\" or \"program\" trace of the evaluation of g(3,2,1) \n",
    "-- a = 3 b = 2 c = 1 d = a * math.sin(b) = 2.728 return 2.728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- \"Forward mode\" augmentation of the above program trace, for calculation of dd/da \n",
    "-- a = 3 da = 1 b = 2 db = 0 c = 1 dc = 0 d = a * math.sin(b) = 2.728 dd = math.sin(b) = 0.909 return 0.909"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- \"Reverse mode\" augmentation of the above program trace, for calculation of dd/da \n",
    "-- a = 3 b = 2 c = 1 d = a * math.sin(b) = 2.728 dd = 1 da = dd * math.sin(b) = 0.909 return 0.909, 2.728"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "Neural networks in Torch can be constructed using the `nn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'nn';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Modules` are the bricks used to build neural networks. Each are themselves neural networks, but can be combined with other networks using `containers` to create complex neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, look at this network that classfies digit images:\n",
    "![LeNet](http://fastml.com/images/cifar/lenet5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a simple feed-forward network.   \n",
    "It takes the input, feeds it through several layers one after the other, and then finally gives the output.\n",
    "\n",
    "Such a network container is `nn.Sequential` which feeds the input through several layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "\n",
    "-- 1 input image channel, 6 output channels, 5x5 convolution kernel\n",
    "net:add(nn.SpatialConvolution(1, 6, 5, 5))\n",
    "\n",
    "-- A max-pooling operation that looks at 2x2 windows and finds the max.\n",
    "net:add(nn.SpatialMaxPooling(2,2,2,2))\n",
    "\n",
    "-- non-linearity\n",
    "net:add(nn.Tanh())\n",
    "\n",
    "-- additional layers\n",
    "net:add(nn.SpatialConvolution(6, 16, 5, 5))\n",
    "net:add(nn.SpatialMaxPooling(2,2,2,2))\n",
    "net:add(nn.Tanh())\n",
    "\n",
    "-- reshapes from a 3D tensor of 16x5x5 into 1D tensor of 16*5*5\n",
    "net:add(nn.View(16*5*5))\n",
    "\n",
    "-- fully connected layers (matrix multiplication between input and weights)\n",
    "net:add(nn.Linear(16*5*5, 120))\n",
    "net:add(nn.Tanh())\n",
    "net:add(nn.Linear(120, 84))\n",
    "net:add(nn.Tanh())\n",
    "\n",
    "-- 10 is the number of outputs of the network (10 classes)\n",
    "net:add(nn.Linear(84, 10))\n",
    "print('Lenet5\\n', tostring(net));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other examples of nn containers are shown in the figure below:\n",
    "![containers](https://raw.githubusercontent.com/soumith/ex/gh-pages/assets/nn_containers.png)\n",
    "\n",
    "Every neural network module in torch has automatic differentiation.\n",
    "It has a `:forward(input)` function that computes the output for a given input, flowing the input through the network.\n",
    "and it has a `:backward(input, gradient)` function that will differentiate each neuron in the network w.r.t. the gradient that is passed in. This is done via the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = torch.rand(1,32,32) -- pass a random tensor as input to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = net:forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net:zeroGradParameters() -- zero the internal gradient buffers of the network (will come to this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradInput = net:backward(input, torch.rand(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(#gradInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can then update the parameters with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net:updateParameters(0.001) -- provide a learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion: Defining a loss function\n",
    "When you want a model to learn to do something, you give it feedback on how well it is doing. This function that computes an objective measure of the model's performance is called a __loss function__.\n",
    "\n",
    "A typical loss function takes in the model's output and the groundtruth and computes a value that quantifies the model's performance.\n",
    "\n",
    "The model then corrects itself to have a smaller loss.\n",
    "\n",
    "In Torch, loss functions are implemented just like neural network modules, and have automatic differentiation.  \n",
    "They have two functions\n",
    "  - `forward(input, target)`\n",
    "  - `backward(input, target)`\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- a negative log-likelihood criterion for multi-class classification\n",
    "criterion = nn.CrossEntropyCriterion()\n",
    "\n",
    "-- let's say the groundtruth was class number: 3\n",
    "criterion:forward(output, 3)\n",
    "gradients = criterion:backward(output, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradInput = net:backward(input, gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Review of what you learnt so far\n",
    "* Network can have many layers of computation\n",
    "* Network takes an input and produces an output in the `:forward` pass\n",
    "* Criterion computes the loss of the network, and it's gradients w.r.t. the output of the network.\n",
    "* Network takes an (input, gradients) pair in it's `:backward` pass and calculates the gradients w.r.t. each layer (and neuron) in the network.\n",
    "\n",
    "##### Missing details\n",
    "> A neural network layer can have learnable parameters or not.\n",
    "\n",
    "A convolution layer learns it's convolution kernels to adapt to the input data and the problem being solved.  \n",
    "A max-pooling layer has no learnable parameters. It only finds the max of local windows.\n",
    "\n",
    "A layer in torch which has learnable weights, will typically have fields .weight (and optionally, .bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = nn.SpatialConvolution(1,3,2,2) -- learn 3 2x2 kernels\n",
    "print(m.weight) -- initially, the weights are randomly initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(m.bias) -- The operation in a convolution layer is: output = convolution(input,weight) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also two other important fields in a learnable layer. The gradWeight and gradBias.\n",
    "The gradWeight accumulates the gradients w.r.t. each weight in the layer, and the gradBias, w.r.t. each bias in the layer.\n",
    "\n",
    "#### Training the network\n",
    "\n",
    "For the network to adjust itself, it typically does this operation (if you do Stochastic Gradient Descent):\n",
    "\n",
    "> weight = weight - learningRate * gradWeight [equation 1]\n",
    "\n",
    "This update over time will adjust the network weights such that the output loss is decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now it is time to discuss one missing piece. Who visits each layer in your neural network and updates the weight according to Equation 1?\n",
    "  - You can do your own training loop\n",
    "    - Pro: easy customization for complicated network\n",
    "    - Con: code duplication\n",
    "    \n",
    "  - You can use existing packages\n",
    "    - [optim](https://github.com/torch/optim)\n",
    "    - nn.StochasticGradient\n",
    "\n",
    "We shall use the simple SGD trainer shipped with the neural network module: [__nn.StochasticGradient__](https://github.com/torch/nn/blob/master/doc/training.md#stochasticgradientmodule-criterion).\n",
    "\n",
    "It has a function :train(dataset) that takes a given dataset and simply trains your network by showing different samples from your dataset to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about data?\n",
    "Generally, when you have to deal with image, text, audio or video data, you can use standard functions like: [__image.load__](https://github.com/torch/image#res-imageloadfilename-depth-tensortype) or [__audio.load__](https://github.com/soumith/lua---audio#usage) to load your data into a _torch.Tensor_ or a Lua table, as convenient.\n",
    "\n",
    "Let us now use some simple data to train our network.\n",
    "\n",
    "We shall use the CIFAR-10 dataset, which has the classes: 'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'.  \n",
    "The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
    "![CIFAR-10 image](https://raw.githubusercontent.com/soumith/ex/gh-pages/assets/cifar10.png)\n",
    "\n",
    "The dataset has 50,000 training images and 10,000 test images in total.\n",
    "\n",
    "__We now have 5 steps left to do in training our first torch neural network__\n",
    "1. Load and normalize data\n",
    "2. Define a Neural Network\n",
    "3. Define Loss function\n",
    "4. Train network on training data\n",
    "5. Test network on test data.\n",
    "\n",
    "#### 1. Load and normalize data\n",
    "\n",
    "Today, in the interest of time, we prepared the data before-hand into a 4D torch ByteTensor of size 10000x3x32x32 (training) and 10000x3x32x32 (testing)\n",
    "Let us download the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.execute('wget -c https://s3.amazonaws.com/torch7/data/cifar10torchsmall.zip')\n",
    "os.execute('unzip -o cifar10torchsmall.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's inspect it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainset = torch.load('cifar10-train.t7')\n",
    "testset = torch.load('cifar10-test.t7')\n",
    "classes = {'airplane', 'automobile', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(#trainset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, let us display an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itorch.image(trainset.data[100]) -- display the 100-th image in dataset\n",
    "print(classes[trainset.label[100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to prepare the dataset to be used with __nn.StochasticGradient__, a couple of things have to be done according to it's [documentation](https://github.com/torch/nn/blob/master/doc/training.md#traindataset).\n",
    "1. The dataset has to have a :size() function.\n",
    "2. The dataset has to have a [i] index operator, so that dataset[i] returns the ith sample in the datset.\n",
    "\n",
    "Both can be done quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- ignore setmetatable() for now, it is a feature beyond the scope of this tutorial.\n",
    "-- It sets the index operator.\n",
    "\n",
    "setmetatable(trainset, \n",
    "    {__index = function(t, i) \n",
    "                    return {\n",
    "                        t.data[i],\n",
    "                        t.label[i]\n",
    "                    } \n",
    "                end}\n",
    ");\n",
    "\n",
    "function trainset:size() \n",
    "    return self.data:size(1) \n",
    "end\n",
    "\n",
    "-- converts the data from a ByteTensor to a DoubleTensor.\n",
    "trainset.data = trainset.data:double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(trainset:size()) -- just to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(trainset[33]) -- load sample number 33.\n",
    "itorch.image(trainset[33][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__One of the most important things you can do in conditioning your data (in general in data-science or machine learning) is to make your data to have a mean of 0.0 and standard-deviation of 1.0.__\n",
    "\n",
    "Let us do that as a final step of our data processing.\n",
    "\n",
    "We are going to do a per-channel normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- remember: our dataset is #samples x #channels x #height x #width\n",
    "-- this picks {all images, 1st channel, all vertical pixels, all horizontal pixels}\n",
    "redChannel = trainset.data:select(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(#redChannel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving back to mean-subtraction and standard-deviation based scaling, doing this operation is simple, using the indexing operator that we learnt above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = {} -- store the mean, to normalize the test set in the future\n",
    "stdv  = {} -- store the standard-deviation for the future\n",
    "for i=1,3 do -- over each image channel\n",
    "    mean[i] = trainset.data:select(2, 1):mean() -- mean estimation\n",
    "    print('Channel ' .. i .. ', Mean: ' .. mean[i])\n",
    "    trainset.data:select(2, 1):add(-mean[i]) -- mean subtraction\n",
    "    \n",
    "    stdv[i] = trainset.data:select(2, i):std() -- std estimation\n",
    "    print('Channel ' .. i .. ', Standard Deviation: ' .. stdv[i])\n",
    "    trainset.data:select(2, i):div(stdv[i]) -- std scaling\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training data is now normalized and ready to be used.\n",
    "\n",
    "#### 2. Time to define our neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use here a LeNet-like network, with 3 input channels and threshold units (ReLU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net:add(nn.SpatialConvolution(3, 6, 5, 5))\n",
    "net:add(nn.SpatialMaxPooling(2,2,2,2))\n",
    "net:add(nn.Threshold())\n",
    "\n",
    "net:add(nn.SpatialConvolution(6, 16, 5, 5))\n",
    "net:add(nn.SpatialMaxPooling(2,2,2,2))\n",
    "net:add(nn.Threshold())\n",
    "\n",
    "net:add(nn.View(16*5*5))\n",
    "\n",
    "net:add(nn.Linear(16*5*5, 120))\n",
    "net:add(nn.Threshold())\n",
    "net:add(nn.Linear(120, 84))\n",
    "net:add(nn.Threshold())\n",
    "net:add(nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Let us define the Loss function\n",
    "\n",
    "Let us use the cross-entropy classification loss. It is well suited for most classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyCriterion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Train the neural network\n",
    "\n",
    "This is when things start to get interesting.  \n",
    "Let us first define an __nn.StochasticGradient__ object. Then we will give our dataset to this object's ___:train___ function, and that will get the ball rolling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer = nn.StochasticGradient(net, criterion)\n",
    "trainer.learningRate = 0.001\n",
    "trainer.maxIteration = 5 -- just do 5 epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer:train(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Test the network, print accuracy\n",
    "\n",
    "We have trained the network for 5 passes over the training dataset.  \n",
    "But we need to check if the network has learnt anything at all.  \n",
    "We will check this by predicting the class label that the neural network outputs, and checking it against the ground-truth. If the prediction is correct, we add the sample to the list of correct predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, first step. Let us display an image from the test set to get familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classes[testset.label[100]])\n",
    "itorch.image(testset.data[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are done with that, let us normalize the test data with the mean and standard-deviation from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testset.data = testset.data:double()   -- convert from Byte tensor to Double tensor\n",
    "for i=1,3 do -- over each image channel\n",
    "    local channel = testset.data:select(2, i)\n",
    "    channel:add(-mean[i]) -- mean subtraction\n",
    "    channel:div(stdv[i]) -- std scaling\n",
    "    print(string.format('channel %d: mean = %f stdv = %f', i, channel:mean(), channel:std()))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- for fun, print the mean and standard-deviation of example-100\n",
    "horse = testset.data[100]\n",
    "print(horse:mean(), horse:std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let us see what the neural network thinks these examples above are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classes[testset.label[100]])\n",
    "itorch.image(testset.data[100])\n",
    "predicted = net:forward(testset.data[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- show scores\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the network predictions. The network assigned a probability to each classes, given the image.\n",
    "\n",
    "To make it clearer, let us tag each probability with it's class-name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i=1,predicted:size(1) do\n",
    "    print(classes[i], predicted[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, fine. How many in total seem to be correct over the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i=1,10000 do\n",
    "    local groundtruth = testset.label[i]\n",
    "    local prediction = net:forward(testset.data[i])\n",
    "    local confidences, indices = torch.sort(prediction, true)  -- true means sort in descending order\n",
    "    if groundtruth == indices[1] then\n",
    "        correct = correct + 1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(correct, 100*correct/10000 .. ' % ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks waaay better than chance, which is 10% accuracy (randomly picking a class out of 10 classes). Seems like the network learnt something.\n",
    "\n",
    "Hmmm, what are the classes that performed well, and the classes that did not perform well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_performance = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0}\n",
    "for i=1,10000 do\n",
    "    local groundtruth = testset.label[i]\n",
    "    local prediction = net:forward(testset.data[i])\n",
    "    local confidences, indices = torch.sort(prediction, true)  -- true means sort in descending order\n",
    "    if groundtruth == indices[1] then\n",
    "        class_performance[groundtruth] = class_performance[groundtruth] + 1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i=1,#classes do\n",
    "    print(classes[i], 100*class_performance[i]/1000 .. ' %')\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so what next? How do we run this neural network on GPUs?\n",
    "\n",
    "#### cunn: neural networks on GPUs using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'cunn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is pretty simple. Take a neural network, and transfer it over to GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = net:cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, transfer the criterion to GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = criterion:cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainset.data = trainset.data:cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's train on GPU :) #sosimple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer = nn.StochasticGradient(net, criterion)\n",
    "trainer.learningRate = 0.001\n",
    "trainer.maxIteration = 5 -- just do 5 epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer:train(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why dont we notice MASSIVE speedup compared to CPU?\n",
    "Because your network is realllly small (and because my laptop sux). \n",
    "\n",
    "**Exercise:** Try increasing the size of the network (argument 1 and 2 of nn.SpatialConvolution(...), see what kind of speedup you get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Goals achieved:__\n",
    "  * Understand torch and the neural networks package at a high-level.\n",
    "  * Train a small neural network on CPU and GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where do I go next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Build crazy graphs of networks, without writing any graphs explicitly: https://github.com/twitter/autograd\n",
    "* Train on imagenet with multiple GPUs: https://github.com/soumith/imagenet-multiGPU.torch\n",
    "* Train recurrent networks with LSTM on text: https://github.com/wojzaremba/lstm\n",
    "\n",
    "* More demos and tutorials: https://github.com/torch/torch7/wiki/Cheatsheet\n",
    "\n",
    "* Chat with developers of Torch: http://gitter.im/torch/torch7\n",
    "* Ask for help: http://groups.google.com/forum/#!forum/torch7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
